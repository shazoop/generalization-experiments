{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST classifier",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNOY6vnsxkt6cDWbu7ecX9l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shazoop/generalization-experiments/blob/main/MNIST_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oj35NHx_YLmP"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67_XTYIhYVdL",
        "outputId": "2d03a75e-62a3-4b98-cbe1-1548a38d40d8"
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize(.3,.3)]\n",
        ")\n",
        "\n",
        "#One time for downloading the MNIST dataset. Don't need to download this again a second time.\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train = True , download = True, transform = transform)\n",
        "testset = torchvision.datasets.MNIST(root='./data', train = False , download = True, transform = transform)\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
            "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIbhcnPFcaiQ"
      },
      "source": [
        "#Set the batch size\n",
        "\n",
        "batch_size = 300"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMkPwb6AZfIC"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPxnMpJFdzua"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzLuMOjXZr_U"
      },
      "source": [
        "class net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.fc1 = nn.Linear(9216, 128)\n",
        "        self.fc2 = nn.Linear(128, 9)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rp5vRc-gvGl"
      },
      "source": [
        "def train_dbl(print_interval, model, device, train_loader, optimizer, epoch, num, num_alt):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        ix1 = (target == num)\n",
        "        ix2 = (target == num_alt)\n",
        "        ix = torch.logical_or(ix1,ix2)\n",
        "        # ix = (target != num)\n",
        "        data, target = data[ix], target[ix]\n",
        "        target[target == num] = 0\n",
        "        target[target == num_alt] = 1\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % print_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "\n",
        "def test_dbl(model, device, test_loader, num,num_alt):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            # ix = (target != num)\n",
        "            ix1 = (target == num)\n",
        "            ix2 = (target == num_alt)\n",
        "            ix = torch.logical_or(ix1,ix2)\n",
        "            data, target = data[ix], target[ix]\n",
        "            target[target == num] = 0\n",
        "            target[target == num_alt] = 1\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    ix1 = test_loader.dataset.targets == num\n",
        "    ix2 = test_loader.dataset.targets == num_alt\n",
        "    ix = torch.logical_or(ix2, ix1)\n",
        "    data_size = torch.sum(ix)\n",
        "    test_loss /= data_size\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, data_size,\n",
        "        100. * correct /data_size))\n",
        "\n",
        "\n",
        " \n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHB4uW0-VqC4"
      },
      "source": [
        "def train(print_interval, model, device, train_loader, optimizer, epoch, num, num_alt):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        ix = (target != num_alt)\n",
        "        data, target = data[ix], target[ix]\n",
        "        target[target == num] = -1\n",
        "        target[target > num] -= 1\n",
        "        target[target > (num_alt - (num_alt > num))] -= 1\n",
        "        target += 1\n",
        "        assert torch.max(target).item() == 8\n",
        "        assert torch.min(target).item() == 0\n",
        "        assert target.size(0) == data.size(0)\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % print_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "\n",
        "def test(model, device, test_loader, num, num_alt):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            ix = (target != num_alt)\n",
        "            data, target = data[ix], target[ix]\n",
        "            target[target == num] = -1\n",
        "            target[target > num] -= 1\n",
        "            target[target > (num_alt - (num_alt > num))] -= 1\n",
        "            target += 1\n",
        "            assert torch.max(target).item() == 8\n",
        "            assert torch.min(target).item() == 0\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    data_size = torch.sum(test_loader.dataset.targets != num_alt)\n",
        "    test_loss /= data_size\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, data_size,\n",
        "        100. * correct /data_size))\n",
        "\n",
        "\n",
        " \n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfDDc8symI9-"
      },
      "source": [
        "import torch.optim as optim"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3gfnWjAg73_"
      },
      "source": [
        "device = torch.device(\"cuda\")\n",
        "Model = net().to(device)\n",
        "optimizer = optim.Adam(Model.parameters(), lr=0.001) #e-1\n"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDM6z8d-lLiv"
      },
      "source": [
        "epoch = 3\n",
        "print_interval = 10"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QQhS0zxqEHg"
      },
      "source": [
        "num = 8\n",
        "num_alt = 5"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnSHr1KJwFpY",
        "outputId": "f5038a2e-8d64-465f-fc7f-f68f24b5b126"
      },
      "source": [
        "for eph in range(1, epoch+1):\n",
        "  train(print_interval, Model, device, train_loader, optimizer, eph, num,num_alt)\n",
        "  test(Model,device, test_loader, num, num_alt)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.196214\n",
            "Train Epoch: 1 [2790/60000 (5%)]\tLoss: 0.480815\n",
            "Train Epoch: 1 [5380/60000 (10%)]\tLoss: 0.213993\n",
            "Train Epoch: 1 [7950/60000 (15%)]\tLoss: 0.250552\n",
            "Train Epoch: 1 [10600/60000 (20%)]\tLoss: 0.182607\n",
            "Train Epoch: 1 [13850/60000 (25%)]\tLoss: 0.196207\n",
            "Train Epoch: 1 [16800/60000 (30%)]\tLoss: 0.160527\n",
            "Train Epoch: 1 [18830/60000 (35%)]\tLoss: 0.126895\n",
            "Train Epoch: 1 [22000/60000 (40%)]\tLoss: 0.107343\n",
            "Train Epoch: 1 [25560/60000 (45%)]\tLoss: 0.101963\n",
            "Train Epoch: 1 [26100/60000 (50%)]\tLoss: 0.087991\n",
            "Train Epoch: 1 [30360/60000 (55%)]\tLoss: 0.061461\n",
            "Train Epoch: 1 [33120/60000 (60%)]\tLoss: 0.053413\n",
            "Train Epoch: 1 [35880/60000 (65%)]\tLoss: 0.086097\n",
            "Train Epoch: 1 [37800/60000 (70%)]\tLoss: 0.102917\n",
            "Train Epoch: 1 [40050/60000 (75%)]\tLoss: 0.061378\n",
            "Train Epoch: 1 [44640/60000 (80%)]\tLoss: 0.042635\n",
            "Train Epoch: 1 [46070/60000 (85%)]\tLoss: 0.107503\n",
            "Train Epoch: 1 [49860/60000 (90%)]\tLoss: 0.089307\n",
            "Train Epoch: 1 [51110/60000 (95%)]\tLoss: 0.044715\n",
            "\n",
            "Test set: Average loss: 0.0531, Accuracy: 8951/9108 (98%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.048174\n",
            "Train Epoch: 2 [2760/60000 (5%)]\tLoss: 0.070901\n",
            "Train Epoch: 2 [5460/60000 (10%)]\tLoss: 0.043492\n",
            "Train Epoch: 2 [8250/60000 (15%)]\tLoss: 0.017890\n",
            "Train Epoch: 2 [11080/60000 (20%)]\tLoss: 0.053532\n",
            "Train Epoch: 2 [13400/60000 (25%)]\tLoss: 0.040916\n",
            "Train Epoch: 2 [15840/60000 (30%)]\tLoss: 0.046868\n",
            "Train Epoch: 2 [18690/60000 (35%)]\tLoss: 0.059938\n",
            "Train Epoch: 2 [21520/60000 (40%)]\tLoss: 0.064533\n",
            "Train Epoch: 2 [24570/60000 (45%)]\tLoss: 0.047298\n",
            "Train Epoch: 2 [26800/60000 (50%)]\tLoss: 0.083211\n",
            "Train Epoch: 2 [29260/60000 (55%)]\tLoss: 0.013804\n",
            "Train Epoch: 2 [32400/60000 (60%)]\tLoss: 0.033424\n",
            "Train Epoch: 2 [34840/60000 (65%)]\tLoss: 0.082919\n",
            "Train Epoch: 2 [36680/60000 (70%)]\tLoss: 0.040799\n",
            "Train Epoch: 2 [42300/60000 (75%)]\tLoss: 0.040418\n",
            "Train Epoch: 2 [44480/60000 (80%)]\tLoss: 0.043565\n",
            "Train Epoch: 2 [45220/60000 (85%)]\tLoss: 0.017496\n",
            "Train Epoch: 2 [48960/60000 (90%)]\tLoss: 0.042220\n",
            "Train Epoch: 2 [50350/60000 (95%)]\tLoss: 0.056128\n",
            "\n",
            "Test set: Average loss: 0.0348, Accuracy: 9005/9108 (99%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.031599\n",
            "Train Epoch: 3 [2650/60000 (5%)]\tLoss: 0.042209\n",
            "Train Epoch: 3 [5500/60000 (10%)]\tLoss: 0.007559\n",
            "Train Epoch: 3 [8160/60000 (15%)]\tLoss: 0.014329\n",
            "Train Epoch: 3 [11040/60000 (20%)]\tLoss: 0.009242\n",
            "Train Epoch: 3 [13900/60000 (25%)]\tLoss: 0.017099\n",
            "Train Epoch: 3 [16320/60000 (30%)]\tLoss: 0.005120\n",
            "Train Epoch: 3 [19180/60000 (35%)]\tLoss: 0.047358\n",
            "Train Epoch: 3 [22240/60000 (40%)]\tLoss: 0.028709\n",
            "Train Epoch: 3 [24210/60000 (45%)]\tLoss: 0.034030\n",
            "Train Epoch: 3 [27100/60000 (50%)]\tLoss: 0.047022\n",
            "Train Epoch: 3 [30030/60000 (55%)]\tLoss: 0.055361\n",
            "Train Epoch: 3 [33480/60000 (60%)]\tLoss: 0.013958\n",
            "Train Epoch: 3 [35620/60000 (65%)]\tLoss: 0.047313\n",
            "Train Epoch: 3 [39200/60000 (70%)]\tLoss: 0.013887\n",
            "Train Epoch: 3 [41400/60000 (75%)]\tLoss: 0.019152\n",
            "Train Epoch: 3 [43200/60000 (80%)]\tLoss: 0.018336\n",
            "Train Epoch: 3 [46240/60000 (85%)]\tLoss: 0.042166\n",
            "Train Epoch: 3 [49140/60000 (90%)]\tLoss: 0.028388\n",
            "Train Epoch: 3 [51490/60000 (95%)]\tLoss: 0.022398\n",
            "\n",
            "Test set: Average loss: 0.0365, Accuracy: 9004/9108 (99%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c20Xt5Cikwv6"
      },
      "source": [
        "# for eph in range(1, epoch+1):\n",
        "#   train_dbl(print_interval, Model, device, train_loader, optimizer, eph, num2,num_alt)\n",
        "#   test_dbl(Model,device, test_loader, num2, num_alt)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpvEJfe3l5xS"
      },
      "source": [
        "import copy"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbIjvE6Bsmuz"
      },
      "source": [
        "Model_clone = copy.deepcopy(Model)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrftiJrxtFU6"
      },
      "source": [
        "class transnet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(transnet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, 1)\n",
        "        self.fc1 = nn.Linear(4608, 28*28)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        # x = self.fc2(x)\n",
        "        # x = F.relu(x)\n",
        "        # x = self.fc3(x)\n",
        "        # x = F.relu(x)\n",
        "        x = torch.reshape(x,(-1,1,28,28))\n",
        "        return x\n",
        "\n",
        "\n",
        "class stacknet(nn.Module):\n",
        "    def __init__(self, translator, classifier):\n",
        "        super(stacknet, self).__init__()\n",
        "        self.translator = translator\n",
        "        self.classifier = classifier\n",
        "    def forward(self, x):\n",
        "        x = self.translator(x)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lphoq4fxfIfg"
      },
      "source": [
        "trans = transnet().to(device)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywdiS0XgzS9T"
      },
      "source": [
        "stack = stacknet(trans,Model_clone).to(device)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kz8xvcOVzZG2",
        "outputId": "480c4dbb-f771-480e-8ad7-2f30554b6153"
      },
      "source": [
        "test(stack, device, test_loader, num, num_alt)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 2.2692, Accuracy: 983/9108 (11%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cterdAKPQsC3",
        "outputId": "f19dbcd6-8f3d-4022-d314-dbef32d52e4e"
      },
      "source": [
        "test(Model_clone, device, test_loader, num, num_alt) #works well on trained class"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0365, Accuracy: 9004/9108 (99%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qsMriaXyDx5",
        "outputId": "dd429905-a575-4c8c-f824-3f1e15716bd4"
      },
      "source": [
        "test(Model_clone, device, test_loader, num_alt, num) #and fails on unseen class"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.3083, Accuracy: 8344/9026 (92%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eszMJPv4zkCu"
      },
      "source": [
        "for par in stack.classifier.parameters():\n",
        "  par.requires_grad = False"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_jWaQ5qTlQY"
      },
      "source": [
        "optimizer = optim.Adam(stack.parameters(), lr=1e-3) #e-1"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlUBOwA7fPhk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0725fa5b-eb2b-4698-b643-7504ab1523c4"
      },
      "source": [
        "for eph in range(1, epoch+1):\n",
        "  train(print_interval, stack, device, train_loader, optimizer, eph, num_alt, num)\n",
        "  test(stack,device, test_loader, num_alt, num)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.298140\n",
            "Train Epoch: 1 [2630/60000 (5%)]\tLoss: 0.317677\n",
            "Train Epoch: 1 [5340/60000 (10%)]\tLoss: 0.232765\n",
            "Train Epoch: 1 [8310/60000 (15%)]\tLoss: 0.165344\n",
            "Train Epoch: 1 [10920/60000 (20%)]\tLoss: 0.114740\n",
            "Train Epoch: 1 [13500/60000 (25%)]\tLoss: 0.142213\n",
            "Train Epoch: 1 [15660/60000 (30%)]\tLoss: 0.111316\n",
            "Train Epoch: 1 [19390/60000 (35%)]\tLoss: 0.098823\n",
            "Train Epoch: 1 [21600/60000 (40%)]\tLoss: 0.057890\n",
            "Train Epoch: 1 [24030/60000 (45%)]\tLoss: 0.138716\n",
            "Train Epoch: 1 [26900/60000 (50%)]\tLoss: 0.046653\n",
            "Train Epoch: 1 [29480/60000 (55%)]\tLoss: 0.057619\n",
            "Train Epoch: 1 [31320/60000 (60%)]\tLoss: 0.090371\n",
            "Train Epoch: 1 [35230/60000 (65%)]\tLoss: 0.031027\n",
            "Train Epoch: 1 [37660/60000 (70%)]\tLoss: 0.047455\n",
            "Train Epoch: 1 [40200/60000 (75%)]\tLoss: 0.073152\n",
            "Train Epoch: 1 [43520/60000 (80%)]\tLoss: 0.043576\n",
            "Train Epoch: 1 [46070/60000 (85%)]\tLoss: 0.046235\n",
            "Train Epoch: 1 [48240/60000 (90%)]\tLoss: 0.056594\n",
            "Train Epoch: 1 [51490/60000 (95%)]\tLoss: 0.073041\n",
            "\n",
            "Test set: Average loss: 0.0514, Accuracy: 8880/9026 (98%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.051024\n",
            "Train Epoch: 2 [2580/60000 (5%)]\tLoss: 0.018710\n",
            "Train Epoch: 2 [5480/60000 (10%)]\tLoss: 0.025499\n",
            "Train Epoch: 2 [8400/60000 (15%)]\tLoss: 0.027155\n",
            "Train Epoch: 2 [11000/60000 (20%)]\tLoss: 0.069602\n",
            "Train Epoch: 2 [13250/60000 (25%)]\tLoss: 0.019501\n",
            "Train Epoch: 2 [16320/60000 (30%)]\tLoss: 0.019034\n",
            "Train Epoch: 2 [19040/60000 (35%)]\tLoss: 0.026694\n",
            "Train Epoch: 2 [20880/60000 (40%)]\tLoss: 0.009187\n",
            "Train Epoch: 2 [24570/60000 (45%)]\tLoss: 0.022846\n",
            "Train Epoch: 2 [26900/60000 (50%)]\tLoss: 0.036393\n",
            "Train Epoch: 2 [29480/60000 (55%)]\tLoss: 0.070358\n",
            "Train Epoch: 2 [32520/60000 (60%)]\tLoss: 0.040432\n",
            "Train Epoch: 2 [35880/60000 (65%)]\tLoss: 0.053635\n",
            "Train Epoch: 2 [37660/60000 (70%)]\tLoss: 0.033507\n",
            "Train Epoch: 2 [40650/60000 (75%)]\tLoss: 0.049830\n",
            "Train Epoch: 2 [43680/60000 (80%)]\tLoss: 0.006683\n",
            "Train Epoch: 2 [46240/60000 (85%)]\tLoss: 0.024537\n",
            "Train Epoch: 2 [48060/60000 (90%)]\tLoss: 0.025686\n",
            "Train Epoch: 2 [52060/60000 (95%)]\tLoss: 0.073391\n",
            "\n",
            "Test set: Average loss: 0.0446, Accuracy: 8898/9026 (99%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.018147\n",
            "Train Epoch: 3 [2670/60000 (5%)]\tLoss: 0.022573\n",
            "Train Epoch: 3 [5320/60000 (10%)]\tLoss: 0.030145\n",
            "Train Epoch: 3 [8280/60000 (15%)]\tLoss: 0.021069\n",
            "Train Epoch: 3 [11000/60000 (20%)]\tLoss: 0.012637\n",
            "Train Epoch: 3 [13850/60000 (25%)]\tLoss: 0.006634\n",
            "Train Epoch: 3 [15960/60000 (30%)]\tLoss: 0.031150\n",
            "Train Epoch: 3 [18480/60000 (35%)]\tLoss: 0.018086\n",
            "Train Epoch: 3 [21280/60000 (40%)]\tLoss: 0.009601\n",
            "Train Epoch: 3 [24840/60000 (45%)]\tLoss: 0.003703\n",
            "Train Epoch: 3 [26800/60000 (50%)]\tLoss: 0.008919\n",
            "Train Epoch: 3 [30250/60000 (55%)]\tLoss: 0.032595\n",
            "Train Epoch: 3 [32400/60000 (60%)]\tLoss: 0.022404\n",
            "Train Epoch: 3 [34320/60000 (65%)]\tLoss: 0.003390\n",
            "Train Epoch: 3 [37940/60000 (70%)]\tLoss: 0.030504\n",
            "Train Epoch: 3 [40800/60000 (75%)]\tLoss: 0.015183\n",
            "Train Epoch: 3 [43680/60000 (80%)]\tLoss: 0.022357\n",
            "Train Epoch: 3 [47430/60000 (85%)]\tLoss: 0.070796\n",
            "Train Epoch: 3 [48420/60000 (90%)]\tLoss: 0.017650\n",
            "Train Epoch: 3 [51300/60000 (95%)]\tLoss: 0.030906\n",
            "\n",
            "Test set: Average loss: 0.0346, Accuracy: 8920/9026 (99%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHzGVJGhPXE9",
        "outputId": "d7624c32-b2c1-43e4-d404-c4d82fc0c7df"
      },
      "source": [
        "test(Model_clone, device, test_loader, num, num_alt)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0365, Accuracy: 9004/9108 (99%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iksya1fIyWHd",
        "outputId": "722f2595-e20e-4a35-8237-f3880b82e6c4"
      },
      "source": [
        "test(Model_clone, device, test_loader, num_alt, num) #classifier still works perfectly on original but fails on novel class. no change."
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.3083, Accuracy: 8344/9026 (92%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7uyPbr-bZl7"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-ewWTOgcACM"
      },
      "source": [
        "dat, tar = next(iter(test_loader))\n",
        "ix = (tar != num)\n",
        "dat, tar = dat[ix], tar[ix]\n",
        "tar[tar == num_alt] = -1\n",
        "tar[tar > num_alt] -= 1\n",
        "tar[tar > (num - (num > num_alt))] -= 1\n",
        "tar += 1\n",
        "\n"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "oZrjgcCLcekP",
        "outputId": "373a5e6d-efba-4d85-fd84-78fbc073f4d5"
      },
      "source": [
        "ex_ix = torch.argmin(tar).item()\n",
        "plt.imshow(dat[ex_ix].view(28,28))"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f1466d98890>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSIuXbCl0brj",
        "outputId": "ed15fb3f-cabe-45df-8f28-aa68b551eaaa"
      },
      "source": [
        "print(num,num_alt)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "XMX9qRBQcqFK",
        "outputId": "8afd46d5-4909-4038-8e0d-555d2e542a36"
      },
      "source": [
        "out_dat = stack.translator(dat.to(device))\n",
        "out_dat_cpu = out_dat.cpu().detach()\n",
        "plt.imshow(out_dat_cpu[ex_ix].view(28,28))"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f1466d84290>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAUg0lEQVR4nO3dfXBV5Z0H8O8vN4FIACGAIYQAggGLWsFGFNdWd5ytL9OudLvD6KwdcGxxW51td23V2lbtrlut68vYGcfdWF/Qdn3ZWpV17bbIdnxhEQiWdzRgeE1CIARMeAnk3vvbP3Jxo+b5neSce++58Hw/M5kk93fPuU8O98u59z7neR5RVRDRya8o7gYQUX4w7ESeYNiJPMGwE3mCYSfyRHE+H2yQDNZSlIXfgYi7FtCrIMUJs67JlP3YQ09x1w4esbcNkBxtH5PitkOR9h/F0QlDzPrgHYfz1JKBS5e7j2v3qWlz29KmpFnXY92h2tQvQ4znGgAcdj/funAIx/Ron0GJFHYRuQLAIwASAH6pqvdZ9y9FGS6Qy8I/3uDBzpoePWpumxg5yqyn2vaZdZ0xw92upavNbYO0fX22WR9dtyzS/qNo+OEssz71Oyvy1JKB67z8Qmdtz192mdtOu63NrCd37grVpv6Qs88x67pynbO2XJc4a6FfxotIAsCjAK4EMB3AtSIyPez+iCi3orxnnwVgi6o2quoxAM8DuDo7zSKibIsS9ioAO3v9vitz2yeIyAIRqReR+m7YL7WJKHdy/mm8qtapaq2q1pbA/Z6biHIrStibAFT3+n185jYiKkBRwr4SQI2InC4igwBcA2BRdppFRNkWuutNVZMicjOA36On6+1JVd0QpTGJmslmPbW5MfS+g7rWgjR/0d3fXLU00q5j7VoLlMM3esUTq816cvtOs56+ZKZZH/biSnftefu6ipZv292hYx6zu96KT59o1pNbtztrumqjue2Bb7jblvqvd91tMvcaQFVfB/B6lH0QUX7wclkiTzDsRJ5g2Ik8wbATeYJhJ/IEw07kCcnn7LLDpVyjDHGlvunsc501WbYmjy0ZmK6v2sNn911vj+M/3Glffl0z770Bt+m4zY+4h8cCwNCJH5n1yjmbQj92FMt1CTq0vc/x7DyzE3mCYSfyBMNO5AmGncgTDDuRJxh2Ik8UVNfb/vn2sMIjp7mnku4abf8dNU/bQ1yl3e5KSe5uNeuW4sqxZl2H2VNJt110mlkf+bR7iOzOH19kblt9z/+a9RPZjjvdf/u4pfYUaYPa7Cmy02uida0lpk911lIbG+xtx4xx1pa1/wYfde9h1xuRzxh2Ik8w7ESeYNiJPMGwE3mCYSfyBMNO5ImC6mcPNMu9uuXhKntp4eEr7al/0wFTTae77FU/iQoBh7gSEcNO5AuGncgTDDuRJxh2Ik8w7ESeYNiJPBFpFde8W7HOWbJ72YFkdltywjgyx56u+ZRXVuSpJX5JDB9u1lMdHXlqyf+LFHYR2QagE0AKQFJVa7PRKCLKvmyc2f9cVduysB8iyiG+ZyfyRNSwK4A/iMgqEVnQ1x1EZIGI1ItIfTfseb+IKHeivoy/WFWbROQ0AItF5H1Vfav3HVS1DkAd0DMQJuLjEVFIkc7sqtqU+b4HwMsA7I9+iSg2ocMuImUiMuz4zwC+DGB9thpGRNkV5WV8BYCXReT4fv5dVf87SmMSI04166kD9tzuFim2/1RN2j3xidGjnLWt35lmbjvhH3M7N3vLLe750SsfPHHnhW/+vj3n/bgHwv9tDf92vlmXpH0eHPOuXR/xrHsu/7iEDruqNgJwLwxORAWFXW9EnmDYiTzBsBN5gmEn8gTDTuSJghriGqVrLUhQ11qQrTe5u9cm/DTe7i2re61oxnRz2+4RpWa9ZP8Rsx516WJLlK61IGVj7CWZz6vcadbfLrG7WyV9oVkf3ug+rkX19jHV7mNm3bnfUFsR0QmHYSfyBMNO5AmGncgTDDuRJxh2Ik8w7ESeyG8/e9kpwNnuZZcT7QfNzVNbtma7Rf0mqdztu/362Wa9/KnwwyXTqzea9a2/mmnWz7jO7vMtKrX76aMsdV30+TPtfa9936w33u8+rpP/yj6mrWYVmIqVAfewtf6de/hu8kv2JM3dw90TPh179F1njWd2Ik8w7ESeYNiJPMGwE3mCYSfyBMNO5AmGncgTee1nT5cU4XCVe3HlIcaSzACAnmmr+6bRFpvZP9/u666+J/zY6qbb7CmR0+fby/eWPxX6oQOdcd2fIm0fpR89SPLhQ2a96DJ7+8m3hr8+ofMaezz6sOfd/dn9UfUfjc7atvmTzW27q9zj2bXEnQOe2Yk8wbATeYJhJ/IEw07kCYadyBMMO5EnGHYiT+S1n73owCEMeXm5s15cOdbcPtmyO9tN+ti+c+1++pER9l31c7uPPmhp4l132PXxP4tv3vqGx2aZ9anfXuGsBS2jXXSZPXd7LnWOt8+DwyLu33ouj783/PO8Xd3z4Qee2UXkSRHZIyLre91WLiKLRWRz5nuULBBRHvTnZfzTAK741G23A1iiqjUAlmR+J6ICFhh2VX0LQPunbr4awMLMzwsBzMlyu4goy8K+Z69Q1ZbMz7sBVLjuKCILACwAgFK4r4snotyK/Gm8qioA56dbqlqnqrWqWluCwVEfjohCChv2VhGpBIDM9z3ZaxIR5ULYsC8CMC/z8zwAr2anOUSUK6IB48BF5DkAlwIYjZ7ptO8C8AqAFwFMALAdwFxV/fSHeJ8xXMr1AgkYhOyh4tMnmvVNd40269Meco/7DppbPdc2LzzPWSvZYb+tm/QTezx6239ONevH3nYft3H3R7s2oeUf7Gsfjoy1c5UqSztrNTe5r0UJslyXoEPb+5z4IfADOlW91lFiaolOILxclsgTDDuRJxh2Ik8w7ESeYNiJPJHfJZsD7LjL7s6Y8NPw3SUtt9j7PhzQVTLlB+GnJQ6S3LrdrGtyjFnfdXm5szZubagmZU3NvPdytu/RX20IuEdQPbzKh+zn4u5XPmfWBy0akc3m9AvP7ESeYNiJPMGwE3mCYSfyBMNO5AmGncgTDDuRJwqqn72sye7rTpw1zV3c2eKuARj3pr0scseUoWZ9x53ufvqJv+s0t9WV9lLUQVNon/mLg2Y9yjDWxBmnm/XuSrs/uHXWKWa98sH4prm27PumvUT3qF9Gu66i6np7OujU/k2R9h8Gz+xEnmDYiTzBsBN5gmEn8gTDTuQJhp3IEww7kScKqp99zHP24OvUIfeUyUGOVtj9wcNeeNes773P3S9bdMBu197r7T7dA/bQZ0y+NXdj6VNbtpr1D3/ingoaAGrm2f3o6YtnOGtF76w2t82loS1Js66zzzXrsmyNXR88aMBtOq75B/bcCwcnu9t+9Gfu5wrP7ESeYNiJPMGwE3mCYSfyBMNO5AmGncgTDDuRJwKXbM6mOJdsTl1q9xd3TrCXDx7xTPi+7v3z7H72kQtz148et303uP/2I6f1ubLwx8bfW5hj4QGg6yuzzHrpayvy1JJPspZsDjyzi8iTIrJHRNb3uu1uEWkSkdWZr6uy2WAiyr7+vIx/GsAVfdz+sKrOyHy9nt1mEVG2BYZdVd8C0J6HthBRDkX5gO5mEVmbeZk/0nUnEVkgIvUiUt+NoxEejoiiCBv2xwBMATADQAuAB113VNU6Va1V1doS2B+CEVHuhAq7qraqakpV0wAeB2B/NElEsQsVdhGp7PXr1wCsd92XiApDYD+7iDwH4FIAowG0Argr8/sMAApgG4AbVdWeuB3x9rMXT5pg1rum2GugD2o77Kyl19hzgBcNGWLW97ww3qwHr0NOA5XreeMT084w66kPtoTftzHX/7Idz+Cjrt199rMHTl6hqtf2cfMT/W8aERUCXi5L5AmGncgTDDuRJxh2Ik8w7ESeKKippHNpx1y7e+vgJHtq4c/d0Rz6sdOH3d12ANC+y14W+cDDF5r1M/7engb7RBXUXZrctiP0viv+x+4ptp8NgHzhLLOeWrXBrB+Z474O7ZRX7OGxWlbqLha5z988sxN5gmEn8gTDTuQJhp3IEww7kScYdiJPMOxEnvBmKunIZp3jLO2fPtTc9MgYe8rkcf8SbcrkOJdFTowqN+taPdZZS6/emO3mfELx+CpnLbmrydw2MXWKWU81fBiqTdnw+2b3v+msy3eifk1XuKmkiejkwLATeYJhJ/IEw07kCYadyBMMO5EnGHYiT7Cf/SSw/UX3NQAT567LY0uyK1Ez2aynNjeG3nf6izPNesckY8w4gBHP5m6Z7aCpx635ESIt2UxEJweGncgTDDuRJxh2Ik8w7ESeYNiJPMGwE3nCm3njgxR9/kyznl77fuh9H/r6BWa97KXlZn1LwLzxxZvt8fInrI6DZlmK7aevJt2zv5/3iz+Z2662u+EjS19iPMCbdtuKznY/V2XLUvd2QY0SkWoR+aOIbBSRDSLy3czt5SKyWEQ2Z76PDNoXEcWnPy/jkwBuUdXpAC4EcJOITAdwO4AlqloDYEnmdyIqUIFhV9UWVX0v83MngE0AqgBcDWBh5m4LAczJVSOJKLoBvWcXkUkAZgJYDqBCVY8vmLUbQIVjmwUAFgBAKexrfokod/r9abyIDAXwEoDvqWpH75r2jKbpc0SNqtapaq2q1pZgcKTGElF4/Qq7iJSgJ+i/VtXfZm5uFZHKTL0SwJ7cNJGIsiHwZbyICIAnAGxS1Yd6lRYBmAfgvsz3V3PSwn4qrraXZE7u3GXWt1xndyZUzZjkrO1oHmVuWzPf7loLEmVJ5obHzzfrU7+1MvS+AaDrK+6lhwFg7wz3U6z6HnsK7VSrff5ouu0is1651D0UdPVMe4rt/fNnm/XS/SmzjoDe0GSp+zw7zN4Ukk67i8aQ9f68Z/8zAN8AsE5Ejh+hO9AT8hdF5AYA2wHM7ce+iCgmgWFX1Xfg/n+KM1EQnSB4uSyRJxh2Ik8w7ESeYNiJPMGwE3nipBniGtSPHmTyreGnBq7BtkiPHaeKZcPNeuvsDrNe+toKs1792oCb1G/JgKuvreWqB7/pXkoaAEZeEm2q6MT0qWb9SLV93C2pjQ3OmupRZ41ndiJPMOxEnmDYiTzBsBN5gmEn8gTDTuQJhp3IEydNP3tUR6+0x30fGus+VOVP5W75XgAoHl9l1pO7mpy1oPHqjX8dMM017LH4zbfaY8onPOteVjnZstvctu1Ge0x51VvuPmUAQFHC/djfzO0UaVZfOACUjDjXWWv+vn1Mi7vcteQL7rkPeGYn8gTDTuQJhp3IEww7kScYdiJPMOxEnmDYiTwhaswznW3DpVwvEPeEtDrb3fcIALJsjbv2hbPMbVNDBpn1orftZXITNZOdtW1z7bHR1f9sz4/e/PJ0sz7od6ea9dF1uevn3/kju8836G9r+Ff3vPJTnncvqQwA6WJ78vWSN1aZ9RNV4qxpZj214QNnbbkuQYe293ngeGYn8gTDTuQJhp3IEww7kScYdiJPMOxEnmDYiTwR2M8uItUAngFQAUAB1KnqIyJyN4BvAdibuesdqvq6ta+gfvaDcy802zL0RfdY3Y/+xt521NJms57ctsOsU982P2If9+rF7rXEy9790Nw21bYvVJuOs6690FUbzG33/q09ln5kwzGzXvL2OrOu3fb2li2/mumsNf/4URxtbOqzn70/k1ckAdyiqu+JyDAAq0Rkcab2sKo+MODWElHe9Wd99hYALZmfO0VkEwB76hQiKjgDes8uIpMAzAQ+nqvoZhFZKyJPishIxzYLRKReROq7ETCNEBHlTL/DLiJDAbwE4Huq2gHgMQBTAMxAz5n/wb62U9U6Va1V1doSDM5Ck4kojH6FXURK0BP0X6vqbwFAVVtVNaWqaQCPA3CPeCCi2AWGXUQEwBMANqnqQ71ur+x1t68BWJ/95hFRtvSn6+1iAG8DWAfgeD/KHQCuRc9LeAWwDcCNmQ/znIK63oJYU+yOe8AeakknnkTFaWY91brHrBfy88XqZh7+/gFz2/Ta9501a4hrfz6NfwdAXxubfepEVFh4BR2RJxh2Ik8w7ESeYNiJPMGwE3mCYSfyREEt2bzjTnva4on31jtrO39obzv+XrtftfF+e0hjSad7WuPqf4rWZ7t/nv3Y7efY10KUNbn/z+6YZk/XXPWGPV1z2W/sJZujKB5bYd8h4V5yGQC23WMftynPtDprKfuRc84aru0eFBwNz+xEnmDYiTzBsBN5gmEn8gTDTuQJhp3IEww7kSfyumSziOwFsL3XTaMBtOWtAQNTqG0r1HYBbFtY2WzbRFUd01chr2H/zIOL1KtqbWwNMBRq2wq1XQDbFla+2saX8USeYNiJPBF32OtifnxLobatUNsFsG1h5aVtsb5nJ6L8ifvMTkR5wrATeSKWsIvIFSLygYhsEZHb42iDi4hsE5F1IrJaRNwD6PPTlidFZI+IrO91W7mILBaRzZnvfa6xF1Pb7haRpsyxWy0iV8XUtmoR+aOIbBSRDSLy3cztsR47o115OW55f88uIgkADQD+AsAuACsBXKuqG/PaEAcR2QagVlVjvwBDRL4E4CCAZ1T17Mxt9wNoV9X7Mv9RjlTV2wqkbXcDOBj3Mt6Z1Yoqey8zDmAOgPmI8dgZ7ZqLPBy3OM7sswBsUdVGVT0G4HkAV8fQjoKnqm8BaP/UzVcDWJj5eSF6nix552hbQVDVFlV9L/NzJ4Djy4zHeuyMduVFHGGvArCz1++7UFjrvSuAP4jIKhFZEHdj+lDRa5mt3QAC5nbKu8BlvPPpU8uMF8yxC7P8eVT8gO6zLlbV8wBcCeCmzMvVgqQ978EKqe+0X8t450sfy4x/LM5jF3b586jiCHsTgOpev4/P3FYQVLUp830PgJdReEtRtx5fQTfz3V7dMI8KaRnvvpYZRwEcuziXP48j7CsB1IjI6SIyCMA1ABbF0I7PEJGyzAcnEJEyAF9G4S1FvQjAvMzP8wC8GmNbPqFQlvF2LTOOmI9d7Mufq2revwBchZ5P5D8E8KM42uBo12QAazJfG+JuG4Dn0POyrhs9n23cAGAUgCUANgN4A0B5AbXtWfQs7b0WPcGqjKltF6PnJfpaAKszX1fFfeyMduXluPFyWSJP8AM6Ik8w7ESeYNiJPMGwE3mCYSfyBMNO5AmGncgT/wcZAlxJlVFlmwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTxvnyMRhrAS"
      },
      "source": [
        "out = stack.classifier(out_dat)\n",
        "pred = out.argmax(dim=1, keepdim=True)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e-q7KXLi-LU",
        "outputId": "dc363ab0-a4f3-4e2f-ad22-6fc877d358d5"
      },
      "source": [
        "pred.squeeze(1).cpu() == tar"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True, False,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True, False,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True, False,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True, False,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tymb-fGa1Crk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rc7a9Wcy1MQr"
      },
      "source": [
        "The translator net won't learn to morph \"num_alt\" into \"num\", even when trained to correctly classify all but \"num_alt\" originally. INTUITION: given train classifier, partition pixel space into classification regions. Then,translator is just learning to map \"num_alt\" into the classification region of \"num\", and this region is not specific enough to just digits of \"num\". Include lots of other stuff. NEXT: perhaps this is a good way to check if classifier really \"learns\" the class. If one can learn a map from some random pixel space to classification region of a certain class, if the region is truly attuned to the class then the learned map should generate instances of that class (that appear so to the human observer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtsZ5z6Z1-sZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}